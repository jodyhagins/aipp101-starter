guard_prefix=WJH_CHAT
namespace = wjh::chat
auto_hash=true
auto_ostream=true
auto_format=true

# API key for authentication with OpenRouter
[class ApiKey]
description=std::string; <=>, non_empty
forward=substr

# Model identifier (e.g., anthropic/claude-sonnet-4)
[class ModelId]
description=std::string; <=>, non_empty
default_value="anthropic/claude-sonnet-4"

# Maximum tokens for response
[class MaxTokens]
description=std::uint32_t; <=>, positive
default_value=4096u

# Temperature for LLM response randomness (0.0 to 2.0)
[class Temperature]
description=float

# System prompt for LLM instructions
[class SystemPrompt]
description=std::string; <=>

# Whether to display help text and exit
[class ShowHelp]
description=bool; ==, bool
default_value=false

# Whether to display resolved config and exit
[class ShowConfig]
description=bool; ==, bool
default_value=false

# Program name for help text and usage messages
[class ProgramName]
description=std::string; <=>

# Formatted help/usage text for the program
[class HelpText]
description=std::string; <=>

# User input message text
[class UserInput]
description=std::string; <=>

# Assistant response text
[class AssistantResponse]
description=std::string; <=>

# Number of tokens in the prompt
[class PromptTokens]
description=std::uint32_t; +, <=>
default_value=0u

# Number of tokens in the completion
[class CompletionTokens]
description=std::uint32_t; +, <=>
default_value=0u

# Total number of tokens used
[class TotalTokens]
description=std::uint32_t; +, <=>
default_value=0u
